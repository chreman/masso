{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook can be used to explore the dataset of policy documents with some prepared text mining/natural language procesing algorithms.\n",
    "\n",
    "It expects that a data frame has been produced by either the default pipeline or a custom analysis pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"results/test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['availableLanguages', 'category', 'classification', 'fulltext',\n",
       "       'identifier', 'keywords', 'lastUpdateDate', 'links', 'local_source',\n",
       "       'publicationDate', 'referenceDocument', 'relatedProject', 'title',\n",
       "       'typesOfAction', 'links2', 'entities', 'title_mentions',\n",
       "       'identifier_mentions', 'cites', 'targets', 'target_links'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51 Null values in availableLanguages.\n",
      "51 Null values in category.\n",
      "0 Null values in classification.\n",
      "0 Null values in fulltext.\n",
      "45 Null values in identifier.\n",
      "51 Null values in keywords.\n",
      "51 Null values in lastUpdateDate.\n",
      "0 Null values in links.\n",
      "0 Null values in local_source.\n",
      "45 Null values in publicationDate.\n",
      "51 Null values in referenceDocument.\n",
      "97 Null values in relatedProject.\n",
      "0 Null values in title.\n",
      "106 Null values in typesOfAction.\n",
      "0 Null values in links2.\n",
      "0 Null values in entities.\n",
      "0 Null values in title_mentions.\n",
      "0 Null values in identifier_mentions.\n",
      "0 Null values in cites.\n",
      "0 Null values in targets.\n",
      "0 Null values in target_links.\n"
     ]
    }
   ],
   "source": [
    "for col in df.columns:\n",
    "    nancount = len(df[df[col].isnull()])\n",
    "    print(\"%d Null values in %s.\" %(nancount, col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/anaconda3/envs/masso/lib/python3.5/site-packages/gensim/utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n"
     ]
    }
   ],
   "source": [
    "# Importing libraries for natural language processing\n",
    "from gensim import corpora\n",
    "from gensim import corpora, models, similarities\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stop_en = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional stopwords can simply be added by filling in the list in the following cell, e.g.:\n",
    "\n",
    "`stop_en.extend([\"extra_stopword1\", \"extra_stopword2\"])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stop_en.extend([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " 'should',\n",
       " 'now']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get a list of all fulltexts from the dataframe\n",
    "fulltexts = df['fulltext']\n",
    "fulltexts = [\" \".join(ft) for ft in fulltexts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove multiple linebreaks from fulltexts, this helps cleaning up artifacts from pdf extraction\n",
    "fulltexts = [ft.replace('\\n  \\n', '\\n') for ft in fulltexts]\n",
    "fulltexts = [ft.replace('\\n \\n', '\\n') for ft in fulltexts]\n",
    "fulltexts = [ft.replace('\\n\\n', '\\n') for ft in fulltexts]\n",
    "fulltexts = [ft.replace('\\n', ' ') for ft in fulltexts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# lowercase and tokenize texts, returns a list of tokens (words) per fulltext\n",
    "texts = [[word for word in nltk.wordpunct_tokenize(document.lower()) if word not in stop_en] for document in fulltexts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# remove all tokens that appear only once in the whole corpus\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "texts = [[token for token in text if frequency[token] > 1] for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a dictionary over all texts (ID to str map)\n",
    "dictionary = corpora.Dictionary(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a corpus by mapping all tokens to their IDs\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a Term Frequency - Inverse Document Frequency model of the corpus\n",
    "tfidf = models.TfidfModel(corpus)\n",
    "corpus_tfidf = tfidf[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  [('\\uf0b7', -0.13126052013077597),\n",
       "   ('.', -0.12462082921383288),\n",
       "   (',', -0.11985459460860363),\n",
       "   ('data', -0.10288781968707131),\n",
       "   ('horizon', -0.10146479199789606),\n",
       "   ('/', -0.10145454974097236),\n",
       "   ('2020', -0.10135921124084254),\n",
       "   ('proposals', -0.10036331252236239),\n",
       "   ('infrastructures', -0.096603621379800539),\n",
       "   ('proposal', -0.096594014436754916)]),\n",
       " (1,\n",
       "  [('macumba', -0.57531005627455789),\n",
       "   ('marine', -0.22720120006666888),\n",
       "   ('macumbaproject', -0.21782464808876306),\n",
       "   ('marieke', -0.21560031347444025),\n",
       "   ('aquatt', -0.19523377957400453),\n",
       "   ('microorganisms', -0.16555206412545889),\n",
       "   ('cultivation', -0.13343474407835504),\n",
       "   ('event', -0.11924132053192454),\n",
       "   ('facebook', -0.11322741468755125),\n",
       "   ('reuver', -0.11271105379290404)]),\n",
       " (2,\n",
       "  [('\\uf0b7', -0.27679017063958128),\n",
       "   ('data', 0.14045364462342363),\n",
       "   ('macumba', -0.13263725954595929),\n",
       "   ('cloud', 0.12630168320046431),\n",
       "   ('\\uf02d', -0.12304616448136178),\n",
       "   ('eloise', 0.11381576148086849),\n",
       "   ('infrastructures', 0.10768616933682662),\n",
       "   ('rda', 0.10424209233302908),\n",
       "   ('entities', -0.10211342333776488),\n",
       "   ('eudat', 0.10037886118865709)]),\n",
       " (3,\n",
       "  [('eloise', -0.4785163767538278),\n",
       "   ('coastal', -0.28446229618499658),\n",
       "   ('zone', -0.26391570334153081),\n",
       "   ('ocean', -0.1934213609662282),\n",
       "   ('meeting', -0.18020864474950821),\n",
       "   ('land', -0.16262977177075005),\n",
       "   ('1999', -0.15430502604783508),\n",
       "   ('noordwijkerhout', -0.14949911485434023),\n",
       "   ('interaction', -0.12960547212793402),\n",
       "   ('studies', -0.11325910087432318)]),\n",
       " (4,\n",
       "  [('entities', 0.18625311199676231),\n",
       "   ('eloise', -0.18582244217012295),\n",
       "   ('\\uf0b7', -0.15858054241735142),\n",
       "   ('legal', 0.14506089486734394),\n",
       "   ('proposal', -0.14011102050117252),\n",
       "   ('shall', 0.13206001815286164),\n",
       "   ('submission', -0.12686466587107018),\n",
       "   ('•', -0.12529002939443984),\n",
       "   ('helpdesk', -0.11564728908747532),\n",
       "   ('republic', 0.11211246433824078)]),\n",
       " (5,\n",
       "  [('\\uf0b7', 0.36546637225006745),\n",
       "   ('helpdesk', -0.17300781925239322),\n",
       "   ('eloise', -0.12522856724253648),\n",
       "   ('score', 0.11423135510107953),\n",
       "   ('legal', -0.1109549399992225),\n",
       "   ('submission', -0.1077034081372833),\n",
       "   ('•', 0.10219916603495316),\n",
       "   ('proposed', 0.097591381131884786),\n",
       "   ('criterion', 0.095331328410038857),\n",
       "   ('cen', -0.094589196782603566)]),\n",
       " (6,\n",
       "  [('2017', 0.19335903317977099),\n",
       "   ('grid', -0.19114026163413875),\n",
       "   ('2016', 0.16516709294247858),\n",
       "   ('/', 0.13629635030135881),\n",
       "   ('eudat', -0.1349377700461597),\n",
       "   ('seafood', 0.13275182854689968),\n",
       "   ('egee', -0.11623444702449783),\n",
       "   ('costs', -0.10854329639648944),\n",
       "   ('\\uf02d', -0.10147189314350008),\n",
       "   ('horizon', 0.098761332197014912)]),\n",
       " (7,\n",
       "  [('seafood', 0.31554070211138385),\n",
       "   ('\\uf0b7', -0.24143807895514871),\n",
       "   ('ecsafeseafood', 0.20610541047244851),\n",
       "   ('safety', 0.18470865712720005),\n",
       "   ('o', 0.15346567205576214),\n",
       "   ('n', 0.13648935657343203),\n",
       "   ('rda', -0.12962432209364086),\n",
       "   ('shall', 0.12812113054324817),\n",
       "   ('islands', -0.1165733598507336),\n",
       "   ('eudat', -0.11436253107878193)]),\n",
       " (8,\n",
       "  [('seafood', 0.29168262854919852),\n",
       "   ('ecsafeseafood', 0.19461554069574505),\n",
       "   ('islands', 0.15543379571500526),\n",
       "   ('costs', -0.15368107638140682),\n",
       "   ('safety', 0.15008429957715622),\n",
       "   ('republic', 0.13689825190372965),\n",
       "   ('entities', 0.13561742394827303),\n",
       "   ('\\uf02d', -0.12930520490708097),\n",
       "   ('legal', 0.11338947889269296),\n",
       "   ('2017', -0.10740656388902181)]),\n",
       " (9,\n",
       "  [('grid', 0.28835961771493285),\n",
       "   ('o', -0.24460533680586766),\n",
       "   ('seafood', 0.24197853477977199),\n",
       "   ('egee', 0.19736886575909382),\n",
       "   ('rri', -0.17670849157228022),\n",
       "   ('ecsafeseafood', 0.15764575360532887),\n",
       "   ('hnscicloud', -0.13669365714051213),\n",
       "   ('cloud', -0.13197156783994785),\n",
       "   ('n', -0.13103132068785464),\n",
       "   ('safety', 0.12796072462729874)])]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create an Latent Semantic Analysis (dimensionality reduction) model over the TF-IDF corpus\n",
    "# vary the num_topics parameter\n",
    "lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=10)\n",
    "corpus_lsi = lsi[corpus_tfidf]\n",
    "lsi.show_topics(num_words=10, formatted=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-348.751590666\n",
      "-311.190748096\n",
      "-286.283611344\n",
      "-274.865774027\n",
      "-268.212354255\n"
     ]
    }
   ],
   "source": [
    "# create an Latent Dirichlet Allocation model over the TF-IDF corpus\n",
    "# (probabilistic distributions of words over topics, and of topics over documents) \n",
    "# vary the num_topics parameter\n",
    "train_corpus = corpus_tfidf[:-20]\n",
    "test_corpus = corpus_tfidf[-20:]\n",
    "for k in [2, 5, 10, 15, 20]:\n",
    "    lda = models.LdaModel(train_corpus, num_topics=k, id2word=dictionary)\n",
    "    corpus_lda = lda[corpus_tfidf]\n",
    "    #lda.show_topics(num_words=10, formatted=False)\n",
    "    print(lda.log_perplexity(test_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# split each fulltext into sentences, word tokenize each sentence\n",
    "sentences = list(chain.from_iterable([sent_tokenize(ft) for ft in fulltexts]))\n",
    "sentences = [word_tokenize(s) for s in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bigram_transformer = models.Phrases(sentences, min_count=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/anaconda3/envs/masso/lib/python3.5/site-packages/gensim/models/phrases.py:248: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "# create a word2vec model based on sentence, positioning words in a vector space\n",
    "w2vModel = models.Word2Vec(bigram_transformer[sentences], size=100, window=5, min_count=3, workers=4, iter=50, sg=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word2vec model can be used to look for words that are possible synonyms, or appear in a very similar context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('industry_conference', 0.7805285453796387),\n",
       " ('debates', 0.7726726531982422),\n",
       " ('While_some', 0.7689239978790283),\n",
       " ('Will', 0.76511150598526),\n",
       " ('two-day', 0.7640762329101562),\n",
       " ('presentations', 0.7594132423400879),\n",
       " ('data-driven', 0.7590374946594238),\n",
       " ('audience', 0.7581926584243774),\n",
       " ('RRI', 0.7555072903633118),\n",
       " ('meeting', 0.7529993653297424),\n",
       " ('ELOISE', 0.7508211135864258),\n",
       " ('motivation', 0.7496086359024048),\n",
       " ('inherently', 0.7488611340522766),\n",
       " ('skill', 0.7477912902832031),\n",
       " ('ideas', 0.7459557056427002),\n",
       " ('schooling', 0.7439967393875122),\n",
       " ('formal_technology', 0.7429242730140686),\n",
       " ('events', 0.7409762740135193),\n",
       " ('exciting', 0.7407679557800293),\n",
       " ('MIDAS', 0.7397767901420593)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2vModel.most_similar_cosmul(topn=20, positive=['open_science'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8610271132859918"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2vModel.n_similarity(['open','science'], ['open','research'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
